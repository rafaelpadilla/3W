{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be897dda",
   "metadata": {},
   "source": [
    "# üìö Classification Fundamentals and Problem Formulation (5 minutes)\n",
    "\n",
    "### What is Supervised Classification?\n",
    "\n",
    "**Supervised Classification** is a machine learning task where we train algorithms to predict discrete class labels for new data based on labeled training examples.\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **Training Data**: Labeled examples (X, y) where X = features, y = class labels\n",
    "2. **Features**: Input variables that describe each sample\n",
    "3. **Labels**: Target classes we want to predict\n",
    "4. **Model**: Algorithm that learns the mapping from features to labels\n",
    "5. **Evaluation**: Metrics to measure how well our model performs\n",
    "\n",
    "### Our Classification Problem: 3W Oil Well Fault Detection\n",
    "\n",
    "- **Objective**: Classify oil well operational states from sensor data\n",
    "- **Input Features**: Time series sensor measurements (flattened into feature vectors)\n",
    "- **Output Classes**: Different types of operational faults (0=normal, 1-9=different fault types)\n",
    "- **Challenge**: Multi-class classification with imbalanced classes\n",
    "\n",
    "### Why Classification Matters in Oil Wells:\n",
    "- **Early Fault Detection**: Prevent costly equipment failures\n",
    "- **Operational Safety**: Avoid dangerous situations\n",
    "- **Maintenance Planning**: Schedule repairs before critical failures\n",
    "- **Cost Reduction**: Minimize downtime and repair costs\n",
    "\n",
    "### Problem Characteristics:\n",
    "- **Multi-class**: 10 different classes (0-9)\n",
    "- **Time Series**: Sequential sensor measurements\n",
    "- **High Dimensional**: Many sensors √ó time steps = many features\n",
    "- **Imbalanced**: Some fault types are much rarer than others\n",
    "- **Real-world**: Noisy, complex industrial data\n",
    "\n",
    "Let's explore different algorithms to solve this classification challenge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce29f117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Loading 3W Dataset for Supervised Classification\n",
      "=======================================================\n",
      "üì¶ Importing modules... ‚úÖ\n",
      "üìÇ Initializing data persistence... ‚úÖ\n",
      "‚ö° Using format: pickle for maximum speed\n",
      "üìÅ Checking windowed directory: processed_data\\cv_splits\\windowed... ‚úÖ\n",
      "üîç Looking for fold directories... ‚úÖ Found 3 folds\n",
      "üìä Loading windowed data from ALL 3 folds for classification...\n",
      "üìÅ Processing fold_1... ‚úÖ\n",
      "üìÅ Processing fold_2... ‚úÖ\n",
      "üìÅ Processing fold_3... ‚úÖ\n",
      "‚úÖ Successfully loaded windowed data from ALL folds!\n",
      "üöÇ Training windows: 505336\n",
      "üß™ Test windows: 78491\n",
      "‚ö° Loading time: 94.042 seconds\n",
      "üìã Processing first training window... ‚úÖ\n",
      "\n",
      "ü™ü Sample Training Window (Window #1):\n",
      "   ‚Ä¢ Shape: (300, 4)\n",
      "   ‚Ä¢ Class: 0\n",
      "   ‚Ä¢ Features: ['P-PDG_scaled', 'P-TPT_scaled', 'T-TPT_scaled', 'class']\n",
      "\n",
      "üìä Training Set Class Distribution:\n",
      "   ‚Ä¢ Class 0: 52936 windows\n",
      "   ‚Ä¢ Class 1: 80335 windows\n",
      "   ‚Ä¢ Class 2: 5796 windows\n",
      "   ‚Ä¢ Class 3: 46257 windows\n",
      "   ‚Ä¢ Class 4: 10236 windows\n",
      "   ‚Ä¢ Class 5: 130401 windows\n",
      "   ‚Ä¢ Class 6: 368 windows\n",
      "   ‚Ä¢ Class 7: 64760 windows\n",
      "   ‚Ä¢ Class 8: 35732 windows\n",
      "   ‚Ä¢ Class 9: 78515 windows\n",
      "\n",
      "üìä Test Set Class Distribution:\n",
      "   ‚Ä¢ Class 0: 26468 windows\n",
      "   ‚Ä¢ Class 1: 10325 windows\n",
      "   ‚Ä¢ Class 2: 618 windows\n",
      "   ‚Ä¢ Class 3: 1875 windows\n",
      "   ‚Ä¢ Class 4: 5118 windows\n",
      "   ‚Ä¢ Class 5: 1326 windows\n",
      "   ‚Ä¢ Class 6: 184 windows\n",
      "   ‚Ä¢ Class 7: 20284 windows\n",
      "   ‚Ä¢ Class 8: 7051 windows\n",
      "   ‚Ä¢ Class 9: 5242 windows\n",
      "\n",
      "‚ö° Performance Summary:\n",
      "   ‚Ä¢ Total execution time: 103.290 seconds\n",
      "   ‚Ä¢ Data loading time: 94.042 seconds\n",
      "   ‚Ä¢ File format: pickle\n",
      "   ‚Ä¢ Folds processed: 3\n",
      "\n",
      "üéØ Dataset Summary for Supervised Classification:\n",
      "   ‚Ä¢ Total training windows: 505336\n",
      "   ‚Ä¢ Total test windows: 78491\n",
      "   ‚Ä¢ Window dimensions: (300, 4)\n",
      "   ‚Ä¢ Classes available: [np.str_('0'), np.str_('1'), np.str_('2'), np.str_('3'), np.str_('4'), np.str_('5'), np.str_('6'), np.str_('7'), np.str_('8'), np.str_('9')]\n",
      "   ‚Ä¢ Ready for: Decision Trees, SVM, Neural Networks\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LOAD 3W DATASET FOR SUPERVISED CLASSIFICATION\n",
    "# ============================================================\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"ü§ñ Loading 3W Dataset for Supervised Classification\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Import data loading utilities\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "print(\"üì¶ Importing modules...\", end=\" \")\n",
    "from src.data_persistence import DataPersistence\n",
    "from src import config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"‚úÖ\")\n",
    "\n",
    "try:\n",
    "    print(\"üìÇ Initializing data persistence...\", end=\" \")\n",
    "    persistence = DataPersistence(base_dir=config.PROCESSED_DATA_DIR, verbose=False)\n",
    "    print(\"‚úÖ\")\n",
    "\n",
    "    print(f\"‚ö° Using format: {config.SAVE_FORMAT} for maximum speed\")\n",
    "\n",
    "    # Check if windowed directory exists\n",
    "    windowed_dir = os.path.join(persistence.cv_splits_dir, \"windowed\")\n",
    "    print(f\"üìÅ Checking windowed directory: {windowed_dir}...\", end=\" \")\n",
    "\n",
    "    if not os.path.exists(windowed_dir):\n",
    "        print(\"‚ùå\")\n",
    "        print(\n",
    "            \"‚ùå No windowed data directory found. Please run Data Treatment notebook first to generate windowed time series data.\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"‚úÖ\")\n",
    "\n",
    "        # Look for fold directories\n",
    "        print(\"üîç Looking for fold directories...\", end=\" \")\n",
    "        fold_dirs = [\n",
    "            d\n",
    "            for d in os.listdir(windowed_dir)\n",
    "            if d.startswith(\"fold_\") and os.path.isdir(os.path.join(windowed_dir, d))\n",
    "        ]\n",
    "        fold_dirs.sort()\n",
    "        print(f\"‚úÖ Found {len(fold_dirs)} folds\")\n",
    "\n",
    "        if not fold_dirs:\n",
    "            print(\"‚ùå No fold directories found in windowed data.\")\n",
    "        else:\n",
    "            # Load data from ALL folds for comprehensive classification\n",
    "            print(\n",
    "                f\"üìä Loading windowed data from ALL {len(fold_dirs)} folds for classification...\"\n",
    "            )\n",
    "\n",
    "            all_train_windows = []\n",
    "            all_train_classes = []\n",
    "            all_train_fold_info = []  # Track which fold each training sample comes from\n",
    "            all_test_windows = []\n",
    "            all_test_classes = []\n",
    "            all_test_fold_info = []  # Track which fold each test sample comes from\n",
    "\n",
    "            load_start = time.time()\n",
    "\n",
    "            for fold_name in fold_dirs:\n",
    "                fold_path = os.path.join(windowed_dir, fold_name)\n",
    "                fold_num = fold_name.replace(\"fold_\", \"\")\n",
    "\n",
    "                print(f\"üìÅ Processing {fold_name}...\", end=\" \")\n",
    "\n",
    "                # Load training data\n",
    "                train_pickle = os.path.join(\n",
    "                    fold_path, f\"train_windowed.{config.SAVE_FORMAT}\"\n",
    "                )\n",
    "                train_parquet = os.path.join(fold_path, \"train_windowed.parquet\")\n",
    "\n",
    "                if os.path.exists(train_pickle):\n",
    "                    fold_train_dfs, fold_train_classes = persistence._load_dataframes(\n",
    "                        train_pickle, config.SAVE_FORMAT\n",
    "                    )\n",
    "                    all_train_windows.extend(fold_train_dfs)\n",
    "                    all_train_classes.extend(fold_train_classes)\n",
    "                    all_train_fold_info.extend(\n",
    "                        [fold_name] * len(fold_train_dfs)\n",
    "                    )  # Track fold info\n",
    "                elif os.path.exists(train_parquet):\n",
    "                    fold_train_dfs, fold_train_classes = persistence._load_from_parquet(\n",
    "                        train_parquet\n",
    "                    )\n",
    "                    all_train_windows.extend(fold_train_dfs)\n",
    "                    all_train_classes.extend(fold_train_classes)\n",
    "                    all_train_fold_info.extend(\n",
    "                        [fold_name] * len(fold_train_dfs)\n",
    "                    )  # Track fold info\n",
    "\n",
    "                # Load test data\n",
    "                test_pickle = os.path.join(\n",
    "                    fold_path, f\"test_windowed.{config.SAVE_FORMAT}\"\n",
    "                )\n",
    "                test_parquet = os.path.join(fold_path, \"test_windowed.parquet\")\n",
    "\n",
    "                if os.path.exists(test_pickle):\n",
    "                    fold_test_dfs, fold_test_classes = persistence._load_dataframes(\n",
    "                        test_pickle, config.SAVE_FORMAT\n",
    "                    )\n",
    "                    all_test_windows.extend(fold_test_dfs)\n",
    "                    all_test_classes.extend(fold_test_classes)\n",
    "                    all_test_fold_info.extend(\n",
    "                        [fold_name] * len(fold_test_dfs)\n",
    "                    )  # Track fold info\n",
    "                elif os.path.exists(test_parquet):\n",
    "                    fold_test_dfs, fold_test_classes = persistence._load_from_parquet(\n",
    "                        test_parquet\n",
    "                    )\n",
    "                    all_test_windows.extend(fold_test_dfs)\n",
    "                    all_test_classes.extend(fold_test_classes)\n",
    "                    all_test_fold_info.extend(\n",
    "                        [fold_name] * len(fold_test_dfs)\n",
    "                    )  # Track fold info\n",
    "\n",
    "                print(\"‚úÖ\")\n",
    "\n",
    "            load_time = time.time() - load_start\n",
    "\n",
    "            if all_train_windows and all_test_windows:\n",
    "                print(f\"‚úÖ Successfully loaded windowed data from ALL folds!\")\n",
    "                print(f\"üöÇ Training windows: {len(all_train_windows)}\")\n",
    "                print(f\"üß™ Test windows: {len(all_test_windows)}\")\n",
    "                print(f\"‚ö° Loading time: {load_time:.3f} seconds\")\n",
    "\n",
    "                # Store for further processing\n",
    "                train_dfs = all_train_windows\n",
    "                train_classes = all_train_classes\n",
    "                train_fold_info = all_train_fold_info  # Store fold tracking info\n",
    "                test_dfs = all_test_windows\n",
    "                test_classes = all_test_classes\n",
    "                test_fold_info = all_test_fold_info  # Store fold tracking info\n",
    "\n",
    "                # Display sample window information\n",
    "                if train_dfs:\n",
    "                    print(\"üìã Processing first training window...\", end=\" \")\n",
    "                    first_train_window = train_dfs[0]\n",
    "                    first_train_class = train_classes[0]\n",
    "                    print(\"‚úÖ\")\n",
    "\n",
    "                    print(f\"\\nü™ü Sample Training Window (Window #1):\")\n",
    "                    print(f\"   ‚Ä¢ Shape: {first_train_window.shape}\")\n",
    "                    print(f\"   ‚Ä¢ Class: {first_train_class}\")\n",
    "                    print(f\"   ‚Ä¢ Features: {list(first_train_window.columns)}\")\n",
    "\n",
    "                    # Show class distribution\n",
    "                    print(f\"\\nüìä Training Set Class Distribution:\")\n",
    "                    train_unique, train_counts = np.unique(\n",
    "                        train_classes, return_counts=True\n",
    "                    )\n",
    "                    for cls, count in zip(train_unique, train_counts):\n",
    "                        print(f\"   ‚Ä¢ Class {cls}: {count} windows\")\n",
    "\n",
    "                    print(f\"\\nüìä Test Set Class Distribution:\")\n",
    "                    test_unique, test_counts = np.unique(\n",
    "                        test_classes, return_counts=True\n",
    "                    )\n",
    "                    for cls, count in zip(test_unique, test_counts):\n",
    "                        print(f\"   ‚Ä¢ Class {cls}: {count} windows\")\n",
    "\n",
    "                    total_time = time.time() - start_time\n",
    "                    print(f\"\\n‚ö° Performance Summary:\")\n",
    "                    print(f\"   ‚Ä¢ Total execution time: {total_time:.3f} seconds\")\n",
    "                    print(f\"   ‚Ä¢ Data loading time: {load_time:.3f} seconds\")\n",
    "                    print(f\"   ‚Ä¢ File format: {config.SAVE_FORMAT}\")\n",
    "                    print(f\"   ‚Ä¢ Folds processed: {len(fold_dirs)}\")\n",
    "\n",
    "                    print(f\"\\nüéØ Dataset Summary for Supervised Classification:\")\n",
    "                    print(f\"   ‚Ä¢ Total training windows: {len(train_dfs)}\")\n",
    "                    print(f\"   ‚Ä¢ Total test windows: {len(test_dfs)}\")\n",
    "                    print(f\"   ‚Ä¢ Window dimensions: {first_train_window.shape}\")\n",
    "                    print(f\"   ‚Ä¢ Classes available: {sorted(train_unique)}\")\n",
    "                    print(f\"   ‚Ä¢ Ready for: Decision Trees, SVM, Neural Networks\")\n",
    "\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è No training windows found in any fold\")\n",
    "                    train_dfs = []\n",
    "                    train_classes = []\n",
    "                    train_fold_info = []\n",
    "                    test_dfs = []\n",
    "                    test_classes = []\n",
    "                    test_fold_info = []\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading data: {str(e)}\")\n",
    "    print(f\"\\nüí° Troubleshooting:\")\n",
    "    print(f\"   1. Make sure 'Data Treatment.ipynb' ran completely\")\n",
    "    print(f\"   2. Check if windowed data was saved successfully\")\n",
    "    print(f\"   3. Verify the processed_data directory exists\")\n",
    "    print(f\"   4. Ensure pickle format is available for fast loading\")\n",
    "\n",
    "    # Show directory status\n",
    "    expected_dir = config.PROCESSED_DATA_DIR\n",
    "    print(f\"\\nüìÅ Directory check: {expected_dir}\")\n",
    "    if os.path.exists(expected_dir):\n",
    "        print(f\"‚úÖ Base directory exists\")\n",
    "        windowed_path = os.path.join(expected_dir, \"cv_splits\", \"windowed\")\n",
    "        if os.path.exists(windowed_path):\n",
    "            print(f\"‚úÖ Windowed directory exists\")\n",
    "            try:\n",
    "                contents = os.listdir(windowed_path)\n",
    "                print(f\"üìÑ Contents: {contents}\")\n",
    "            except:\n",
    "                print(\"‚ùå Cannot list directory contents\")\n",
    "        else:\n",
    "            print(f\"‚ùå Windowed directory missing: {windowed_path}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Base directory does not exist\")\n",
    "\n",
    "    # Initialize empty variables for error case\n",
    "    train_dfs = []\n",
    "    train_classes = []\n",
    "    train_fold_info = []\n",
    "    test_dfs = []\n",
    "    test_classes = []\n",
    "    test_fold_info = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e476fc",
   "metadata": {},
   "source": [
    "# üîç Enhanced Analysis Summary\n",
    "\n",
    "## Five Key Analysis Features\n",
    "\n",
    "The enhanced classification provides five critical analysis capabilities:\n",
    "\n",
    "### 1. üéØ **Configurable Class Selection**\n",
    "- **Purpose**: Choose which specific classes to include in your analysis\n",
    "- **Configuration**: Controlled via `src/config.py` for easy management\n",
    "- **Options**: \n",
    "  - Default: Exclude only class 0 (analyze all fault types 1-9)\n",
    "  - Custom: Select specific classes of interest (e.g., [2,3,8] for particular faults)\n",
    "  - Presets: Use pre-configured selections for common scenarios\n",
    "- **Use Case**: Focus on specific fault types, reduce problem complexity, targeted analysis\n",
    "\n",
    "### 2. üìä **Accuracy Per Fold**\n",
    "- **Purpose**: Understand model consistency across different data splits\n",
    "- **Insight**: Identifies if models perform consistently or if some folds are particularly challenging\n",
    "- **Use Case**: Helps detect dataset bias, temporal patterns, or fold-specific issues\n",
    "\n",
    "### 3. üè∑Ô∏è **Accuracy Per Class** \n",
    "- **Purpose**: Understand model performance on each selected class\n",
    "- **Insight**: Reveals which fault types are easier/harder to detect\n",
    "- **Use Case**: Critical for industrial applications where missing certain fault types has higher cost\n",
    "\n",
    "### 4. ‚ö†Ô∏è **Flexible Class Filtering**\n",
    "- **Purpose**: Either exclude normal operation (class 0) or focus on specific fault types\n",
    "- **Insight**: Create focused classification problems aligned with operational needs\n",
    "- **Use Case**: \n",
    "  - Fault diagnosis systems (exclude normal operation)\n",
    "  - Specific fault type analysis (select particular faults)\n",
    "  - Simplified problems for testing and development\n",
    "\n",
    "### 5. üîÑ **Configurable Test Data Balancing**\n",
    "- **Purpose**: Ensure robust evaluation with sufficient samples per class\n",
    "- **Configuration**: Enable/disable via config file settings\n",
    "- **Insight**: Balances test data to have minimum samples per selected class for reliable metrics\n",
    "- **Use Case**: Prevents evaluation bias due to class imbalance in test set\n",
    "\n",
    "## Configuration Management\n",
    "\n",
    "### Config File Location:\n",
    "```\n",
    "üìÅ src/config.py\n",
    "‚îî‚îÄ‚îÄ CLASSIFICATION_CONFIG section\n",
    "‚îî‚îÄ‚îÄ CLASSIFICATION_PRESETS section\n",
    "```\n",
    "\n",
    "### Available Presets:\n",
    "```python\n",
    "CLASSIFICATION_PRESETS = {\n",
    "    'all_faults': None,                # All fault types (exclude class 0)\n",
    "    'specific_faults': [2, 3, 8],     # User-defined specific faults\n",
    "    'early_faults': [1, 2, 3, 4, 5],  # First 5 fault types\n",
    "    'late_faults': [7, 8, 9],         # Last 3 fault types\n",
    "    'odd_faults': [1, 3, 5, 7, 9],    # Odd-numbered fault types\n",
    "    'even_faults': [2, 4, 6, 8],      # Even-numbered fault types\n",
    "    'critical_faults': [3, 6, 8, 9],  # Critical operational faults\n",
    "    'minor_faults': [1, 2, 4, 5, 7],  # Minor operational faults\n",
    "    'binary_test': [3, 8],            # Simple binary classification\n",
    "}\n",
    "```\n",
    "\n",
    "### Configuration Examples:\n",
    "\n",
    "#### In config.py:\n",
    "```python\n",
    "CLASSIFICATION_CONFIG = {\n",
    "    'selected_classes': [2, 3, 8],     # Default selection\n",
    "    'balance_test': False,             # Test balancing setting\n",
    "    'min_test_samples_per_class': 300, # Min samples when balancing\n",
    "    'balance_classes': True,           # Training balancing\n",
    "    'balance_strategy': 'combined',    # Balancing strategy\n",
    "    'max_samples_per_class': 1000,     # Training limit\n",
    "    'verbose': True                    # Progress display\n",
    "}\n",
    "```\n",
    "\n",
    "#### In notebook override:\n",
    "```python\n",
    "# Quick override in notebook\n",
    "selected_classes = config.CLASSIFICATION_PRESETS['binary_test']  # Use preset\n",
    "# OR\n",
    "selected_classes = [1, 4, 7]  # Custom selection\n",
    "```\n",
    "\n",
    "## Why Configuration Management Matters\n",
    "\n",
    "### Easy Experimentation:\n",
    "- **Quick Changes**: Modify config file without editing notebook code\n",
    "- **Preset Library**: Use common configurations instantly\n",
    "- **Version Control**: Track configuration changes easily\n",
    "- **Reproducibility**: Share exact configurations with team\n",
    "\n",
    "### Industrial Relevance:\n",
    "- **Deployment Ready**: Production configurations separate from code\n",
    "- **Operational Flexibility**: Adapt to different operational scenarios\n",
    "- **Maintenance Efficiency**: Update configurations without code changes\n",
    "- **Team Collaboration**: Shared configuration standards\n",
    "\n",
    "### Model Development:\n",
    "- **Systematic Testing**: Compare different class combinations systematically\n",
    "- **Performance Optimization**: Focus computational resources efficiently\n",
    "- **Iterative Development**: Progressive complexity increase\n",
    "- **Configuration Documentation**: Track what works best for different scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "229fa361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Running Enhanced Supervised Classification Analysis\n",
      "============================================================\n",
      "üìä Using enhanced supervised classification module...\n",
      "üöÇ Training windows available: 505336\n",
      "üß™ Test windows available: 78491\n",
      "üìÅ Fold information available: 3 folds detected\n",
      "üìÅ Folds: ['fold_1', 'fold_2', 'fold_3']\n",
      "\n",
      "‚öôÔ∏è Loading classification configuration from config.py...\n",
      "üìã Available classification presets:\n",
      "   ‚Ä¢ all_faults: None\n",
      "   ‚Ä¢ specific_faults: [2, 3, 8]\n",
      "   ‚Ä¢ early_faults: [1, 2, 3, 4, 5]\n",
      "   ‚Ä¢ late_faults: [7, 8, 9]\n",
      "   ‚Ä¢ odd_faults: [1, 3, 5, 7, 9]\n",
      "   ‚Ä¢ even_faults: [2, 4, 6, 8]\n",
      "   ‚Ä¢ critical_faults: [3, 6, 8, 9]\n",
      "   ‚Ä¢ minor_faults: [1, 2, 4, 5, 7]\n",
      "   ‚Ä¢ binary_test: [3, 8]\n",
      "\n",
      "üéØ Current Classification Configuration:\n",
      "   üìä Selected classes: [1, 2, 3, 4, 5, 8, 9]\n",
      "   üìä Balance test data: False\n",
      "   üìä Analysis Mode: Custom class selection\n",
      "   üìä Classes to analyze: [1, 2, 3, 4, 5, 8, 9]\n",
      "\n",
      "üîÑ Starting enhanced classification pipeline...\n",
      "üìã Analysis will include:\n",
      "   üéØ Custom class selection: [1, 2, 3, 4, 5, 8, 9]\n",
      "   ‚úÖ Only selected classes included in analysis\n",
      "   ‚úÖ Standard model training (Decision Trees, SVM, Neural Networks)\n",
      "   ‚úÖ Data augmentation for class balancing (training)\n",
      "   ‚ö†Ô∏è Test data balancing disabled\n",
      "   ‚úÖ Per-class accuracy analysis (selected classes only)\n",
      "   ‚ö†Ô∏è Per-fold analysis (may be disabled due to test balancing)\n",
      "üîÑ Enhanced Classification with Test Balancing and Class 0 Exclusion\n",
      "=================================================================\n",
      "üéØ Filtering to include only selected classes: [1, 2, 3, 4, 5, 8, 9]...\n",
      "‚úÖ Class filtering completed:\n",
      "   ‚Ä¢ Original training samples: 505336 ‚Üí Filtered: 387272\n",
      "   ‚Ä¢ Original test samples: 78491 ‚Üí Filtered: 31555\n",
      "   ‚Ä¢ Remaining training classes: {np.str_('1'): np.int64(80335), np.str_('2'): np.int64(5796), np.str_('3'): np.int64(46257), np.str_('4'): np.int64(10236), np.str_('5'): np.int64(130401), np.str_('8'): np.int64(35732), np.str_('9'): np.int64(78515)}\n",
      "   ‚Ä¢ Remaining test classes: {np.str_('1'): np.int64(10325), np.str_('2'): np.int64(618), np.str_('3'): np.int64(1875), np.str_('4'): np.int64(5118), np.str_('5'): np.int64(1326), np.str_('8'): np.int64(7051), np.str_('9'): np.int64(5242)}\n",
      "üîß Preparing Data for Supervised Classification\n",
      "=======================================================\n",
      "üè∑Ô∏è Processing class labels... ‚úÖ\n",
      "üîÑ Mapping transient classes to base classes... ‚úÖ\n",
      "üè∑Ô∏è Training class distribution: {np.int64(0): np.int64(33061), np.int64(1): np.int64(72561), np.int64(2): np.int64(4700), np.int64(3): np.int64(46257), np.int64(4): np.int64(10236), np.int64(5): np.int64(128652), np.int64(8): np.int64(31230), np.int64(9): np.int64(60575)}\n",
      "üè∑Ô∏è Test class distribution: {np.int64(0): np.int64(7898), np.int64(1): np.int64(8958), np.int64(2): np.int64(358), np.int64(3): np.int64(1875), np.int64(4): np.int64(5118), np.int64(5): np.int64(1110), np.int64(8): np.int64(5529), np.int64(9): np.int64(709)}\n",
      "‚öñÔ∏è Balancing classes using 'combined' strategy... ‚öñÔ∏è Class Balancing Strategy: combined\n",
      "   ‚Ä¢ Original class distribution: {np.int16(0): 33061, 1: 72561, 2: 4700, np.int16(3): 46257, np.int16(4): 10236, 5: 128652, 8: 31230, 9: 60575}\n",
      "üìâ Random Undersampling (strategy: auto)\n",
      "   ‚Ä¢ Original class distribution: {np.int16(0): 33061, 1: 72561, 2: 4700, np.int16(3): 46257, np.int16(4): 10236, 5: 128652, 8: 31230, 9: 60575}\n",
      "   ‚Ä¢ Target class counts: {np.int16(0): 39659, 1: 39659, 2: 39659, np.int16(3): 39659, np.int16(4): 39659, 5: 39659, 8: 39659, 9: 39659}\n",
      "   ‚Ä¢ Final class distribution: {np.int16(0): 33061, np.int16(1): 39659, 2: 4700, np.int16(3): 39659, np.int16(4): 10236, np.int16(5): 39659, 8: 31230, 9: 39659}\n",
      "   ‚Ä¢ Samples: 387272 ‚Üí 237863\n",
      "üìà Random Oversampling (strategy: auto)\n",
      "   ‚Ä¢ Original class distribution: {np.int16(0): 33061, np.int16(1): 39659, 2: 4700, np.int16(3): 39659, np.int16(4): 10236, np.int16(5): 39659, 8: 31230, 9: 39659}\n",
      "   ‚Ä¢ Target class counts: {np.int16(0): 39659, 1: 39659, 2: 39659, np.int16(3): 39659, np.int16(4): 39659, 5: 39659, 8: 39659, 9: 39659}\n",
      "   ‚Ä¢ Final class distribution: {np.int16(0): 39659, np.int16(1): 39659, 2: 39659, np.int16(3): 39659, np.int16(4): 39659, np.int16(5): 39659, 8: 39659, 9: 39659}\n",
      "   ‚Ä¢ Samples: 237863 ‚Üí 317272\n",
      "‚úÖ\n",
      "üéØ Limiting to max 1000 samples per class... ‚úÖ\n",
      "üè∑Ô∏è Final training distribution: {np.int64(0): np.int64(1000), np.int64(1): np.int64(1000), np.int64(2): np.int64(1000), np.int64(3): np.int64(1000), np.int64(4): np.int64(1000), np.int64(5): np.int64(1000), np.int64(8): np.int64(1000), np.int64(9): np.int64(1000)}\n",
      "üè∑Ô∏è Final test distribution: {np.int64(0): np.int64(333), np.int64(1): np.int64(333), np.int64(2): np.int64(333), np.int64(3): np.int64(333), np.int64(4): np.int64(333), np.int64(5): np.int64(333), np.int64(8): np.int64(333), np.int64(9): np.int64(333)}\n",
      "üîÑ Flattening windows to feature vectors... ‚úÖ\n",
      "üè∑Ô∏è Encoding labels... ‚úÖ\n",
      "\n",
      "‚úÖ Data prepared successfully!\n",
      "üöÇ Training set: 8000 samples, 900 features\n",
      "üß™ Test set: 2664 samples, 900 features\n",
      "üè∑Ô∏è Classes: 8 ([np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(8), np.int64(9)])\n",
      "üìä Features per window: 900 (data already normalized)\n",
      "‚ö†Ô∏è Warning: Class 0 detected in prepared data, removing...\n",
      "‚úÖ Final class 0 removal completed:\n",
      "   ‚Ä¢ Training samples after final filter: 7000\n",
      "   ‚Ä¢ Test samples after final filter: 2331\n",
      "   ‚Ä¢ Final training classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7)]\n",
      "   ‚Ä¢ Final test classes: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7)]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along axis 0; size of axis is 31555 but size of corresponding boolean axis is 2664",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 113\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    111\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m   ‚ö†Ô∏è Per-fold analysis (skipped - fold info unavailable)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m classifier = \u001b[43menhanced_fold_analysis\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dfs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Full training data\u001b[39;49;00m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Full training labels\u001b[39;49;00m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_dfs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_dfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Full test data\u001b[39;49;00m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Full test labels\u001b[39;49;00m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfold_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest_fold_info\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfold_available\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass fold info if available\u001b[39;49;00m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbalance_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclassification_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbalance_classes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# From config\u001b[39;49;00m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbalance_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclassification_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbalance_strategy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# From config\u001b[39;49;00m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_samples_per_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclassification_config\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_samples_per_class\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# From config\u001b[39;49;00m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbalance_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbalance_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# From config (configurable)\u001b[39;49;00m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_test_samples_per_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_test_samples_per_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# From config (configurable)\u001b[39;49;00m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mselected_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mselected_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# From config (configurable)\u001b[39;49;00m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclassification_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbose\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# From config\u001b[39;49;00m\n\u001b[32m    130\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müéâ Enhanced Supervised Classification Complete!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    133\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ All models trained and compared\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucas\\Documents\\GitHub\\3W\\resources\\introduction_to_ml_applied_to_mts\\src\\supervised_classification.py:1330\u001b[39m, in \u001b[36menhanced_fold_analysis\u001b[39m\u001b[34m(train_dfs, train_classes, test_dfs, test_classes, fold_info, balance_classes, balance_strategy, max_samples_per_class, balance_test, min_test_samples_per_class, selected_classes, verbose)\u001b[39m\n\u001b[32m   1327\u001b[39m \u001b[38;5;66;03m# Filter fold_info to match the filtered test data\u001b[39;00m\n\u001b[32m   1328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fold_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m test_filter_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1329\u001b[39m     \u001b[38;5;66;03m# Filter fold_info to match the test data after class filtering\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1330\u001b[39m     fold_info = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfold_info\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_filter_mask\u001b[49m\u001b[43m]\u001b[49m.tolist()\n\u001b[32m   1331\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m   1332\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Fold information filtered to match test data\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: boolean index did not match indexed array along axis 0; size of axis is 31555 but size of corresponding boolean axis is 2664"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# COMPREHENSIVE SUPERVISED CLASSIFICATION WITH ENHANCED ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "print(\"ü§ñ Running Enhanced Supervised Classification Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if we have loaded data from previous cell\n",
    "if (\n",
    "    \"train_dfs\" in locals()\n",
    "    and train_dfs is not None\n",
    "    and len(train_dfs) > 0\n",
    "    and \"test_dfs\" in locals()\n",
    "    and test_dfs is not None\n",
    "    and len(test_dfs) > 0\n",
    "):\n",
    "\n",
    "    # Import the enhanced supervised classification module and config\n",
    "    from src.supervised_classification import enhanced_fold_analysis\n",
    "    from src import config\n",
    "\n",
    "    print(\"üìä Using enhanced supervised classification module...\")\n",
    "    print(f\"üöÇ Training windows available: {len(train_dfs)}\")\n",
    "    print(f\"üß™ Test windows available: {len(test_dfs)}\")\n",
    "\n",
    "    # Check if fold information is available\n",
    "    fold_available = (\n",
    "        \"test_fold_info\" in locals()\n",
    "        and test_fold_info is not None\n",
    "        and len(test_fold_info) == len(test_dfs)\n",
    "    )\n",
    "\n",
    "    if fold_available:\n",
    "        unique_folds = sorted(set(test_fold_info))\n",
    "        print(f\"üìÅ Fold information available: {len(unique_folds)} folds detected\")\n",
    "        print(f\"üìÅ Folds: {unique_folds}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Fold information not available - will skip per-fold analysis\")\n",
    "\n",
    "    # ============================================================\n",
    "    # LOAD CLASSIFICATION CONFIGURATION FROM CONFIG FILE\n",
    "    # ============================================================\n",
    "\n",
    "    print(f\"\\n‚öôÔ∏è Loading classification configuration from config.py...\")\n",
    "\n",
    "    # Load configuration from config file\n",
    "    classification_config = config.CLASSIFICATION_CONFIG\n",
    "    classification_presets = config.CLASSIFICATION_PRESETS\n",
    "\n",
    "    # You can easily change the configuration by modifying these lines:\n",
    "    # Option 1: Use configuration directly from config file\n",
    "    selected_classes = classification_config[\"selected_classes\"]\n",
    "    balance_test = classification_config[\"balance_test\"]\n",
    "    min_test_samples_per_class = classification_config[\"min_test_samples_per_class\"]\n",
    "\n",
    "    # Option 2: Override with a preset (uncomment one to use)\n",
    "    # selected_classes = classification_presets['all_faults']         # All fault types\n",
    "    # selected_classes = classification_presets['early_faults']       # Classes 1-5\n",
    "    # selected_classes = classification_presets['late_faults']        # Classes 7-9\n",
    "    # selected_classes = classification_presets['odd_faults']         # Odd classes\n",
    "    # selected_classes = classification_presets['binary_test']        # Classes 3,8\n",
    "\n",
    "    # Option 3: Custom selection (uncomment and modify)\n",
    "    # selected_classes = [1, 4, 7]  # Your custom selection\n",
    "\n",
    "    print(f\"üìã Available classification presets:\")\n",
    "    for preset_name, preset_classes in classification_presets.items():\n",
    "        print(f\"   ‚Ä¢ {preset_name}: {preset_classes}\")\n",
    "\n",
    "    print(f\"\\nüéØ Current Classification Configuration:\")\n",
    "    print(f\"   üìä Selected classes: {selected_classes}\")\n",
    "    print(f\"   üìä Balance test data: {balance_test}\")\n",
    "    if balance_test:\n",
    "        print(f\"   üìä Min test samples per class: {min_test_samples_per_class}\")\n",
    "\n",
    "    if selected_classes is None:\n",
    "        print(f\"   üìä Analysis Mode: All fault types (exclude only class 0)\")\n",
    "        print(f\"   üìä Classes to analyze: All available fault classes (1-9)\")\n",
    "    else:\n",
    "        print(f\"   üìä Analysis Mode: Custom class selection\")\n",
    "        print(f\"   üìä Classes to analyze: {selected_classes}\")\n",
    "\n",
    "    # Run enhanced supervised classification with fold analysis\n",
    "    # This will provide:\n",
    "    # 1. Class filtering based on selected_classes parameter\n",
    "    # 2. Accuracy per fold (if fold info available and test balancing disabled)\n",
    "    # 3. Accuracy per class for selected classes only\n",
    "    # 4. Confirmation of class selection status\n",
    "    # Plus all the standard analysis (model comparison, visualizations, etc.)\n",
    "\n",
    "    print(\"\\nüîÑ Starting enhanced classification pipeline...\")\n",
    "    print(\"üìã Analysis will include:\")\n",
    "    if selected_classes is None:\n",
    "        print(\"   ‚ö†Ô∏è Complete exclusion of class 0 (normal operation)\")\n",
    "        print(\"   ‚úÖ All fault types (classes 1-9) included\")\n",
    "    else:\n",
    "        print(f\"   üéØ Custom class selection: {selected_classes}\")\n",
    "        print(\"   ‚úÖ Only selected classes included in analysis\")\n",
    "    print(\"   ‚úÖ Standard model training (Decision Trees, SVM, Neural Networks)\")\n",
    "    print(\"   ‚úÖ Data augmentation for class balancing (training)\")\n",
    "    if balance_test:\n",
    "        print(\n",
    "            f\"   ‚úÖ Test data balancing (ensures min {min_test_samples_per_class} samples per class)\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è Test data balancing disabled\")\n",
    "    print(\"   ‚úÖ Per-class accuracy analysis (selected classes only)\")\n",
    "    if fold_available:\n",
    "        print(\"   ‚ö†Ô∏è Per-fold analysis (may be disabled due to test balancing)\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è Per-fold analysis (skipped - fold info unavailable)\")\n",
    "\n",
    "    classifier = enhanced_fold_analysis(\n",
    "        train_dfs=train_dfs,  # Full training data\n",
    "        train_classes=train_classes,  # Full training labels\n",
    "        test_dfs=test_dfs,  # Full test data\n",
    "        test_classes=test_classes,  # Full test labels\n",
    "        fold_info=(\n",
    "            test_fold_info if fold_available else None\n",
    "        ),  # Pass fold info if available\n",
    "        balance_classes=classification_config[\"balance_classes\"],  # From config\n",
    "        balance_strategy=classification_config[\"balance_strategy\"],  # From config\n",
    "        max_samples_per_class=classification_config[\n",
    "            \"max_samples_per_class\"\n",
    "        ],  # From config\n",
    "        balance_test=balance_test,  # From config (configurable)\n",
    "        min_test_samples_per_class=min_test_samples_per_class,  # From config (configurable)\n",
    "        selected_classes=selected_classes,  # From config (configurable)\n",
    "        verbose=classification_config[\"verbose\"],  # From config\n",
    "    )\n",
    "\n",
    "    print(f\"\\nüéâ Enhanced Supervised Classification Complete!\")\n",
    "    print(f\"‚úÖ All models trained and compared\")\n",
    "    if selected_classes is None:\n",
    "        print(f\"‚ö†Ô∏è Class 0 (normal operation) completely excluded from analysis\")\n",
    "        print(f\"‚úÖ All fault types (classes 1-9) analyzed\")\n",
    "    else:\n",
    "        print(f\"üéØ Custom class selection applied: {selected_classes}\")\n",
    "        print(f\"‚úÖ Only selected classes analyzed\")\n",
    "    print(f\"‚úÖ Class balancing applied using data augmentation (training)\")\n",
    "    if balance_test:\n",
    "        print(\n",
    "            f\"‚úÖ Test data balanced to ensure robust evaluation (min {min_test_samples_per_class} per class)\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Test data balancing disabled\")\n",
    "    print(f\"‚úÖ Per-class accuracy analysis completed (selected classes only)\")\n",
    "    if fold_available:\n",
    "        print(f\"‚ö†Ô∏è Per-fold accuracy analysis (may be disabled due to test balancing)\")\n",
    "    print(f\"‚úÖ Standard performance visualizations created\")\n",
    "    print(f\"‚úÖ Enhanced analysis provides comprehensive classification insights\")\n",
    "\n",
    "    print(f\"\\nüí° Configuration Tips:\")\n",
    "    print(f\"   ‚Ä¢ Edit 'src/config.py' to change default settings\")\n",
    "    print(f\"   ‚Ä¢ Use classification presets for common scenarios\")\n",
    "    print(f\"   ‚Ä¢ Override settings in this cell for quick experiments\")\n",
    "\n",
    "    # Store classifier for further analysis if needed\n",
    "    supervised_classifier = classifier\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"‚ùå No data available. Please run the previous cell first to load training and test data.\"\n",
    "    )\n",
    "    supervised_classifier = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956fe609",
   "metadata": {},
   "source": [
    "## Ô∏è Configuration-Based Classification (New!)\n",
    "\n",
    "### Easy Configuration Management\n",
    "\n",
    "The classification system now uses a configuration file approach for easy experimentation:\n",
    "\n",
    "**üìÅ Configuration File:** `src/config.py`\n",
    "- Contains all classification settings\n",
    "- Pre-configured presets for common scenarios\n",
    "- Easy to modify without changing notebook code\n",
    "\n",
    "### Quick Configuration Examples:\n",
    "\n",
    "#### 1. Use Default Configuration (Current: [2, 3, 8]):\n",
    "```python\n",
    "# Already configured in config.py\n",
    "selected_classes = config.CLASSIFICATION_CONFIG['selected_classes']\n",
    "```\n",
    "\n",
    "#### 2. Use a Preset:\n",
    "```python\n",
    "selected_classes = config.CLASSIFICATION_PRESETS['binary_test']    # Classes [3, 8]\n",
    "selected_classes = config.CLASSIFICATION_PRESETS['early_faults']   # Classes [1,2,3,4,5]\n",
    "selected_classes = config.CLASSIFICATION_PRESETS['all_faults']     # All fault types (1-9)\n",
    "```\n",
    "\n",
    "#### 3. Custom Selection:\n",
    "```python\n",
    "selected_classes = [1, 4, 7]  # Your specific choice\n",
    "```\n",
    "\n",
    "### Test Data Balancing Control:\n",
    "```python\n",
    "balance_test = True   # Enable balanced test evaluation\n",
    "balance_test = False  # Use original test distribution (faster)\n",
    "```\n",
    "\n",
    "### Benefits:\n",
    "- **üöÄ Quick Experiments**: Change focus without code modifications\n",
    "- **üìã Reproducible**: Share exact configurations\n",
    "- **‚öôÔ∏è Production Ready**: Separate configuration from code\n",
    "- **üîÑ Version Control**: Track configuration changes\n",
    "\n",
    "---\n",
    "\n",
    "## üå≥ Decision Trees and Random Forest (8 minutes)\n",
    "\n",
    "### Decision Trees\n",
    "- **How it works**: Creates a tree-like model of decisions\n",
    "- **Advantages**: Easy to interpret, handles both numerical and categorical data\n",
    "- **Disadvantages**: Prone to overfitting, can be unstable\n",
    "\n",
    "### Random Forest\n",
    "- **How it works**: Combines many decision trees (ensemble method)\n",
    "- **Advantages**: Reduces overfitting, more robust, provides feature importance\n",
    "- **Disadvantages**: Less interpretable than single tree, can still overfit with noisy data\n",
    "\n",
    "### Why Good for Oil Well Data:\n",
    "- Handles high-dimensional features well\n",
    "- Provides feature importance (which sensors are most important)\n",
    "- Robust to outliers and noise\n",
    "- Works well with time series features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4c07b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå≥ Individual Algorithm Training Example\n",
      "==================================================\n",
      "‚ùå No classifier available. Please run the comprehensive classification cell first.\n",
      "üí° The comprehensive cell above handles:\n",
      "   ‚Ä¢ Data preparation and class balancing with augmentation\n",
      "   ‚Ä¢ Training of all algorithms (Decision Trees, SVM, Neural Networks)\n",
      "   ‚Ä¢ Model comparison and visualization\n",
      "   ‚Ä¢ Performance analysis and recommendations\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# INDIVIDUAL ALGORITHM TRAINING (OPTIONAL)\n",
    "# ============================================================\n",
    "\n",
    "print(\"üå≥ Individual Algorithm Training Example\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# This cell demonstrates how to use individual algorithms from the module\n",
    "# The comprehensive classification above already ran all algorithms\n",
    "\n",
    "if \"supervised_classifier\" in locals() and supervised_classifier is not None:\n",
    "    print(\"üìä Comprehensive classification already completed above!\")\n",
    "    print(\"üîç Here's how to access individual algorithm results:\")\n",
    "\n",
    "    # Access results from the comprehensive run\n",
    "    results = supervised_classifier.results\n",
    "\n",
    "    # Find tree-based models\n",
    "    tree_models = [\n",
    "        r for r in results if \"Tree\" in r[\"model_name\"] or \"Forest\" in r[\"model_name\"]\n",
    "    ]\n",
    "\n",
    "    if tree_models:\n",
    "        print(f\"\\nüå≥ Tree-Based Models Performance:\")\n",
    "        print(\"-\" * 40)\n",
    "        for result in tree_models:\n",
    "            print(f\"{result['model_name']}:\")\n",
    "            print(f\"   ‚Ä¢ Training Accuracy: {result['train_accuracy']:.3f}\")\n",
    "            print(f\"   ‚Ä¢ Test Accuracy: {result['test_accuracy']:.3f}\")\n",
    "            print(f\"   ‚Ä¢ Training Time: {result['training_time']:.3f}s\")\n",
    "\n",
    "            # Show feature importance for Random Forest\n",
    "            if (\n",
    "                \"Random Forest\" in result[\"model_name\"]\n",
    "                and \"feature_importance\" in result\n",
    "            ):\n",
    "                print(f\"   ‚Ä¢ Top 5 Most Important Features:\")\n",
    "                feature_importance = result[\"feature_importance\"]\n",
    "                top_features_idx = np.argsort(feature_importance)[-5:][::-1]\n",
    "                for i, idx in enumerate(top_features_idx, 1):\n",
    "                    print(f\"     {i}. Feature {idx}: {feature_importance[idx]:.4f}\")\n",
    "            print()\n",
    "\n",
    "    print(\"üí° To train individual algorithms separately:\")\n",
    "    print(\n",
    "        \"   1. Use supervised_classifier.prepare_data() to get X_train, y_train, X_test, y_test\"\n",
    "    )\n",
    "    print(\n",
    "        \"   2. Call supervised_classifier.train_decision_trees(X_train, y_train, X_test, y_test)\"\n",
    "    )\n",
    "    print(\"   3. Or use supervised_classifier.train_svm() or train_neural_networks()\")\n",
    "\n",
    "    # Example of how to train just decision trees individually\n",
    "    print(f\"\\nüîß Example: Training only Decision Trees individually\")\n",
    "    print(\"(This would be useful if you only want specific algorithms)\")\n",
    "\n",
    "    # Note: Data is already prepared in the classifier\n",
    "    print(\n",
    "        \"‚úÖ Data preparation, class balancing, and augmentation already handled by module\"\n",
    "    )\n",
    "    print(\"‚úÖ All models already trained - see comprehensive results above\")\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"‚ùå No classifier available. Please run the comprehensive classification cell first.\"\n",
    "    )\n",
    "    print(\"üí° The comprehensive cell above handles:\")\n",
    "    print(\"   ‚Ä¢ Data preparation and class balancing with augmentation\")\n",
    "    print(\"   ‚Ä¢ Training of all algorithms (Decision Trees, SVM, Neural Networks)\")\n",
    "    print(\"   ‚Ä¢ Model comparison and visualization\")\n",
    "    print(\"   ‚Ä¢ Performance analysis and recommendations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da46019e",
   "metadata": {},
   "source": [
    "## ‚ö° Support Vector Machines (4 minutes)\n",
    "\n",
    "### How SVM Works:\n",
    "- **Goal**: Find the optimal hyperplane that separates classes with maximum margin\n",
    "- **Kernel Trick**: Map data to higher dimensions to make it linearly separable\n",
    "- **Support Vectors**: Data points closest to the decision boundary\n",
    "\n",
    "### SVM Advantages:\n",
    "- Effective in high-dimensional spaces (perfect for our flattened time series)\n",
    "- Memory efficient (only uses support vectors)\n",
    "- Versatile (different kernels for different data patterns)\n",
    "\n",
    "### SVM for Oil Well Data:\n",
    "- Handles high-dimensional sensor data well\n",
    "- RBF kernel can capture non-linear patterns in sensor readings\n",
    "- Good for binary classification problems (normal vs fault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef0fc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SVM RESULTS FROM COMPREHENSIVE CLASSIFICATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"‚ö° Support Vector Machines Results\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if \"supervised_classifier\" in locals() and supervised_classifier is not None:\n",
    "    print(\"üìä SVM models already trained in comprehensive classification!\")\n",
    "\n",
    "    # Access SVM results\n",
    "    results = supervised_classifier.results\n",
    "    svm_models = [r for r in results if \"SVM\" in r[\"model_name\"]]\n",
    "\n",
    "    if svm_models:\n",
    "        print(f\"\\n‚ö° SVM Performance Summary:\")\n",
    "        print(\"-\" * 35)\n",
    "        for result in svm_models:\n",
    "            print(f\"{result['model_name']}:\")\n",
    "            print(f\"   ‚Ä¢ Training Accuracy: {result['train_accuracy']:.3f}\")\n",
    "            print(f\"   ‚Ä¢ Test Accuracy: {result['test_accuracy']:.3f}\")\n",
    "            print(f\"   ‚Ä¢ Training Time: {result['training_time']:.3f}s\")\n",
    "            print()\n",
    "\n",
    "        # Show which SVM performed better\n",
    "        best_svm = max(svm_models, key=lambda x: x[\"test_accuracy\"])\n",
    "        print(\n",
    "            f\"üèÜ Best SVM: {best_svm['model_name']} (Test Accuracy: {best_svm['test_accuracy']:.3f})\"\n",
    "        )\n",
    "\n",
    "        print(f\"\\n SVM Implementation Details:\")\n",
    "        print(f\"   ‚Ä¢ Used subset sampling for computational efficiency\")\n",
    "        print(f\"   ‚Ä¢ Linear SVM: Faster, good for linearly separable data\")\n",
    "        print(f\"   ‚Ä¢ RBF SVM: Better for complex non-linear patterns\")\n",
    "        print(f\"   ‚Ä¢ Data already normalized - optimal for SVM performance\")\n",
    "        print(f\"   ‚Ä¢ Class balancing with augmentation improved SVM robustness\")\n",
    "\n",
    "    print(f\"\\n Module Benefits:\")\n",
    "    print(f\"   ‚úÖ Automatic subset sampling for large datasets\")\n",
    "    print(f\"   ‚úÖ Both Linear and RBF kernels tested\")\n",
    "    print(f\"   ‚úÖ Optimized hyperparameters\")\n",
    "    print(f\"   ‚úÖ Integrated with data augmentation for class balancing\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No classifier results available.\")\n",
    "    print(\"üí° Run the comprehensive classification cell to see SVM results.\")\n",
    "    print(\"üìã The module automatically handles:\")\n",
    "    print(\"   ‚Ä¢ Subset sampling for SVM computational efficiency\")\n",
    "    print(\"   ‚Ä¢ Training both Linear and RBF SVM variants\")\n",
    "    print(\"   ‚Ä¢ Performance comparison with other algorithms\")\n",
    "    print(\"   ‚Ä¢ Integration with balanced dataset using augmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b91df2",
   "metadata": {},
   "source": [
    "## üß† Neural Networks and Deep Learning Basics (8 minutes)\n",
    "\n",
    "### Neural Networks Fundamentals:\n",
    "- **Neurons**: Basic processing units that compute weighted sums + activation\n",
    "- **Layers**: Input layer ‚Üí Hidden layers ‚Üí Output layer\n",
    "- **Backpropagation**: Learning algorithm that adjusts weights based on errors\n",
    "- **Activation Functions**: Non-linear functions (ReLU, sigmoid, tanh)\n",
    "\n",
    "### Why Neural Networks for Oil Well Data:\n",
    "- **Automatic Feature Learning**: Can discover complex patterns in sensor data\n",
    "- **Non-linear Relationships**: Capture complex interactions between sensors\n",
    "- **Temporal Patterns**: Can model dependencies in time series data\n",
    "- **Scalability**: Handle large amounts of high-dimensional data\n",
    "\n",
    "### Types of Neural Networks:\n",
    "1. **Multi-Layer Perceptron (MLP)**: Standard feedforward network\n",
    "2. **Convolutional Neural Networks (CNN)**: Good for pattern recognition\n",
    "3. **Recurrent Neural Networks (RNN/LSTM)**: Designed for sequential data\n",
    "\n",
    "### Deep Learning Advantages:\n",
    "- Learns features automatically from raw data\n",
    "- Can model very complex relationships\n",
    "- State-of-the-art performance on many tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b809c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# NEURAL NETWORKS RESULTS FROM COMPREHENSIVE CLASSIFICATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"üß† Neural Networks Results\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "if \"supervised_classifier\" in locals() and supervised_classifier is not None:\n",
    "    print(\"üìä Neural Network models already trained in comprehensive classification!\")\n",
    "\n",
    "    # Access Neural Network results\n",
    "    results = supervised_classifier.results\n",
    "    nn_models = [r for r in results if \"Neural Network\" in r[\"model_name\"]]\n",
    "\n",
    "    if nn_models:\n",
    "        print(f\"\\nüß† Neural Networks Performance Summary:\")\n",
    "        print(\"-\" * 45)\n",
    "\n",
    "        # Create comparison table\n",
    "        print(\n",
    "            f\"{'Model':<25} {'Train Acc':<10} {'Test Acc':<10} {'Time (s)':<10} {'Iterations':<12}\"\n",
    "        )\n",
    "        print(\"-\" * 67)\n",
    "\n",
    "        for result in nn_models:\n",
    "            iterations = result.get(\"iterations\", \"N/A\")\n",
    "            print(\n",
    "                f\"{result['model_name']:<25} {result['train_accuracy']:<10.3f} \"\n",
    "                f\"{result['test_accuracy']:<10.3f} {result['training_time']:<10.3f} {iterations:<12}\"\n",
    "            )\n",
    "\n",
    "        # Show best neural network\n",
    "        best_nn = max(nn_models, key=lambda x: x[\"test_accuracy\"])\n",
    "        print(\n",
    "            f\"\\nüèÜ Best Neural Network: {best_nn['model_name']} (Test Accuracy: {best_nn['test_accuracy']:.3f})\"\n",
    "        )\n",
    "\n",
    "        print(f\"\\nüß† Neural Network Architecture Analysis:\")\n",
    "        print(f\"   ‚Ä¢ Simple NN (1 layer): Fast baseline, good for simple patterns\")\n",
    "        print(f\"   ‚Ä¢ Deep NN (3 layers): Complex pattern recognition, may overfit\")\n",
    "        print(f\"   ‚Ä¢ Regularized NN: Balanced approach with dropout prevention\")\n",
    "\n",
    "        print(f\"\\nüí° Key Neural Network Insights:\")\n",
    "        print(f\"   ‚Ä¢ Early stopping prevented overfitting automatically\")\n",
    "        print(f\"   ‚Ä¢ Data already normalized - optimal for neural network training\")\n",
    "        print(f\"   ‚Ä¢ Class balancing improved learning from minority classes\")\n",
    "        print(f\"   ‚Ä¢ Adaptive learning rates helped convergence\")\n",
    "\n",
    "        # Show training efficiency\n",
    "        total_nn_time = sum(r[\"training_time\"] for r in nn_models)\n",
    "        print(f\"\\n‚ö° Training Efficiency:\")\n",
    "        print(f\"   ‚Ä¢ Total NN training time: {total_nn_time:.3f} seconds\")\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Average iterations: {np.mean([r.get('iterations', 0) for r in nn_models]):.1f}\"\n",
    "        )\n",
    "        print(f\"   ‚Ä¢ All models used early stopping for efficiency\")\n",
    "\n",
    "    print(f\"\\n Module Neural Network Features:\")\n",
    "    print(f\"   ‚úÖ Multiple architectures tested automatically\")\n",
    "    print(f\"   ‚úÖ Early stopping and regularization built-in\")\n",
    "    print(f\"   ‚úÖ Optimized hyperparameters for time series data\")\n",
    "    print(f\"   ‚úÖ Integrated with balanced dataset\")\n",
    "    print(f\"   ‚úÖ Automatic feature learning from flattened windows\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No classifier results available.\")\n",
    "    print(\"üí° Run the comprehensive classification cell to see Neural Network results.\")\n",
    "    print(\"üìã The module automatically provides:\")\n",
    "    print(\"   ‚Ä¢ Simple Neural Network (1 hidden layer)\")\n",
    "    print(\"   ‚Ä¢ Deep Neural Network (3 hidden layers)\")\n",
    "    print(\"   ‚Ä¢ Regularized Neural Network (with dropout prevention)\")\n",
    "    print(\"   ‚Ä¢ Early stopping and validation for all networks\")\n",
    "    print(\"   ‚Ä¢ Performance comparison and overfitting analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b6c9c6",
   "metadata": {},
   "source": [
    "## üöÄ Implementation and Training Summary (5 minutes)\n",
    "\n",
    "### Model Performance Comparison\n",
    "Let's compare all the models we trained and understand their strengths and weaknesses for oil well fault detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c0e926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPREHENSIVE ANALYSIS AND TUTORIAL SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "print(\" Supervised Classification Tutorial Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if \"supervised_classifier\" in locals() and supervised_classifier is not None:\n",
    "    print(\"‚úÖ Comprehensive supervised classification completed successfully!\")\n",
    "\n",
    "    # Show final summary of what was accomplished\n",
    "    results = supervised_classifier.results\n",
    "\n",
    "    print(f\"\\nüìä Tutorial Accomplishments:\")\n",
    "    print(f\"   ‚úÖ Loaded windowed data from ALL folds\")\n",
    "    print(f\"   ‚úÖ Applied data augmentation for class balancing\")\n",
    "    print(f\"   ‚úÖ Used existing data normalization (no additional scaling)\")\n",
    "    print(f\"   ‚úÖ Trained {len(results)} different classification models\")\n",
    "    print(f\"   ‚úÖ Compared model performance comprehensively\")\n",
    "    print(f\"   ‚úÖ Generated performance visualizations\")\n",
    "    print(f\"   ‚úÖ Provided practical recommendations\")\n",
    "\n",
    "    # Show algorithm categories covered\n",
    "    tree_models = [\n",
    "        r for r in results if \"Tree\" in r[\"model_name\"] or \"Forest\" in r[\"model_name\"]\n",
    "    ]\n",
    "    svm_models = [r for r in results if \"SVM\" in r[\"model_name\"]]\n",
    "    nn_models = [r for r in results if \"Neural Network\" in r[\"model_name\"]]\n",
    "\n",
    "    print(f\"\\nüî¨ Algorithms Implemented:\")\n",
    "    print(f\"   üå≥ Tree-Based: {len(tree_models)} models (Decision Tree, Random Forest)\")\n",
    "    print(f\"   ‚ö° Support Vector Machines: {len(svm_models)} models (Linear, RBF)\")\n",
    "    print(f\"   üß† Neural Networks: {len(nn_models)} models (Simple, Deep, Regularized)\")\n",
    "\n",
    "    # Show best performers in each category\n",
    "    if tree_models:\n",
    "        best_tree = max(tree_models, key=lambda x: x[\"test_accuracy\"])\n",
    "        print(\n",
    "            f\"\\nüèÜ Best Tree Model: {best_tree['model_name']} ({best_tree['test_accuracy']:.3f})\"\n",
    "        )\n",
    "\n",
    "    if svm_models:\n",
    "        best_svm = max(svm_models, key=lambda x: x[\"test_accuracy\"])\n",
    "        print(\n",
    "            f\"üèÜ Best SVM Model: {best_svm['model_name']} ({best_svm['test_accuracy']:.3f})\"\n",
    "        )\n",
    "\n",
    "    if nn_models:\n",
    "        best_nn = max(nn_models, key=lambda x: x[\"test_accuracy\"])\n",
    "        print(\n",
    "            f\"üèÜ Best Neural Network: {best_nn['model_name']} ({best_nn['test_accuracy']:.3f})\"\n",
    "        )\n",
    "\n",
    "    # Overall best model\n",
    "    overall_best = max(results, key=lambda x: x[\"test_accuracy\"])\n",
    "    print(\n",
    "        f\"\\nü•á Overall Best Model: {overall_best['model_name']} ({overall_best['test_accuracy']:.3f})\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nüí° Key Learning Outcomes Achieved:\")\n",
    "    print(f\"   üìö Classification Fundamentals: Problem formulation and evaluation\")\n",
    "    print(f\"   üå≥ Decision Trees & Random Forest: Interpretable ensemble methods\")\n",
    "    print(f\"   ‚ö° Support Vector Machines: High-dimensional data classification\")\n",
    "    print(f\"   üß† Neural Networks: Deep learning for automatic feature extraction\")\n",
    "    print(f\"   üìä Model Comparison: Performance analysis and selection criteria\")\n",
    "\n",
    "    print(f\"\\nüîß Technical Implementations:\")\n",
    "    print(\n",
    "        f\"   ‚úÖ Data augmentation for class balancing (using src/data_augmentation.py)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   ‚úÖ Comprehensive classification module (src/supervised_classification.py)\"\n",
    "    )\n",
    "    print(f\"   ‚úÖ No redundant normalization (data already preprocessed)\")\n",
    "    print(f\"   ‚úÖ Efficient computational strategies (SVM subsampling)\")\n",
    "    print(f\"   ‚úÖ Early stopping and regularization for neural networks\")\n",
    "\n",
    "    print(f\"\\nüéØ Production Readiness:\")\n",
    "    print(f\"   ‚Ä¢ Model code modularized in src/ folder\")\n",
    "    print(f\"   ‚Ä¢ Class balancing pipeline integrated\")\n",
    "    print(f\"   ‚Ä¢ Performance metrics and visualizations available\")\n",
    "    print(f\"   ‚Ä¢ Best model identified with practical recommendations\")\n",
    "    print(f\"   ‚Ä¢ Ready for deployment and further optimization\")\n",
    "\n",
    "    print(f\"\\nüìà Next Steps:\")\n",
    "    print(f\"   1. Implement cross-validation for more robust evaluation\")\n",
    "    print(f\"   2. Hyperparameter tuning for the best performing algorithms\")\n",
    "    print(f\"   3. Ensemble methods combining multiple top performers\")\n",
    "    print(f\"   4. Real-time inference pipeline development\")\n",
    "    print(f\"   5. Model monitoring and maintenance procedures\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Comprehensive classification not completed.\")\n",
    "    print(\"üí° Please run the comprehensive classification cell to:\")\n",
    "    print(\"   1. Load and prepare windowed data from all folds\")\n",
    "    print(\"   2. Apply data augmentation for class balancing\")\n",
    "    print(\"   3. Train all classification algorithms\")\n",
    "    print(\"   4. Compare model performance\")\n",
    "    print(\"   5. Generate visualizations and recommendations\")\n",
    "\n",
    "print(f\"\\n Supervised Classification Tutorial Complete!\")\n",
    "print(f\"   You have successfully implemented and compared:\")\n",
    "print(f\"   ‚úÖ Decision Trees and Random Forest (8 min)\")\n",
    "print(f\"   ‚úÖ Support Vector Machines (4 min)\")\n",
    "print(f\"   ‚úÖ Neural Networks and Deep Learning (8 min)\")\n",
    "print(f\"   ‚úÖ Implementation and Training Summary (5 min)\")\n",
    "print(f\"   ‚úÖ Comprehensive model comparison and analysis\")\n",
    "print(f\"   ‚úÖ Production-ready modular implementation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "owl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

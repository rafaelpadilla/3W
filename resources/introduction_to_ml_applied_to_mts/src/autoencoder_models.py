"""
LSTM Autoencoder Models for Anomaly Detection

This module provides stable LSTM autoencoder architectures optimized for
numerical stability and time series anomaly detection.
"""

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input,
    LSTM,
    Dense,
    RepeatVector,
    TimeDistributed,
    Dropout,
)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.initializers import GlorotUniform, Orthogonal
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import numpy as np


class StableLSTMAutoencoder:
    """Stable LSTM Autoencoder for time series anomaly detection."""

    def __init__(self, time_steps, n_features, latent_dim=16, lstm_units=32):
        """
        Initialize the LSTM Autoencoder.

        Args:
            time_steps (int): Number of time steps in input sequences
            n_features (int): Number of features per time step
            latent_dim (int): Dimension of the latent representation
            lstm_units (int): Number of LSTM units in encoder/decoder
        """
        self.time_steps = time_steps
        self.n_features = n_features
        self.latent_dim = latent_dim
        self.lstm_units = lstm_units
        self.model = None
        self.history = None

    def build_model(self):
        """Build the stable LSTM autoencoder architecture."""
        print(f"üèóÔ∏è Building Stable LSTM Autoencoder:")
        print(f"   ‚Ä¢ Input shape: ({self.time_steps}, {self.n_features})")
        print(f"   ‚Ä¢ Encoder LSTM units: {self.lstm_units}")
        print(f"   ‚Ä¢ Latent dimension: {self.latent_dim}")
        print(f"   ‚Ä¢ Decoder LSTM units: {self.lstm_units}")

        # Clear any previous models to avoid memory issues
        tf.keras.backend.clear_session()

        # Input layer
        input_layer = Input(
            shape=(self.time_steps, self.n_features), name="input", dtype="float32"
        )

        # Encoder with careful initialization and dropout
        encoded = LSTM(
            self.lstm_units,
            activation="tanh",  # More stable than relu
            recurrent_activation="sigmoid",
            kernel_initializer=GlorotUniform(seed=42),
            recurrent_initializer=Orthogonal(seed=42),
            dropout=0.1,
            recurrent_dropout=0.1,
            name="encoder_lstm",
        )(input_layer)
        encoded = Dropout(0.2, name="encoder_dropout")(encoded)

        # Latent representation (bottleneck)
        latent = Dense(
            self.latent_dim,
            activation="tanh",  # More stable than relu
            kernel_initializer=GlorotUniform(seed=42),
            name="latent",
        )(encoded)

        # Decoder
        decoded = RepeatVector(self.time_steps, name="repeat_vector")(latent)
        decoded = LSTM(
            self.lstm_units,
            activation="tanh",  # More stable than relu
            recurrent_activation="sigmoid",
            kernel_initializer=GlorotUniform(seed=42),
            recurrent_initializer=Orthogonal(seed=42),
            dropout=0.1,
            recurrent_dropout=0.1,
            return_sequences=True,
            name="decoder_lstm",
        )(decoded)
        decoded = Dropout(0.2, name="decoder_dropout")(decoded)
        decoded = TimeDistributed(
            Dense(
                self.n_features,
                kernel_initializer=GlorotUniform(seed=42),
                activation="sigmoid",
            ),  # Sigmoid to match data range
            name="output",
        )(decoded)

        # Create autoencoder model
        self.model = Model(input_layer, decoded, name="stable_lstm_autoencoder")

        # Compile model with stable settings and gradient clipping
        self.model.compile(
            optimizer=Adam(
                learning_rate=0.0005, clipnorm=1.0  # Conservative learning rate
            ),  # Gradient clipping
            loss="mse",
            metrics=["mae"],
        )

        print("‚úÖ Stable LSTM Autoencoder model created")
        print(f"   ‚Ä¢ Total parameters: {self.model.count_params():,}")
        print(f"   ‚Ä¢ Gradient clipping enabled (clipnorm=1.0)")
        print(f"   ‚Ä¢ Conservative learning rate (0.0005)")

        return self.model

    def train(self, train_data, val_data, epochs=30, batch_size=32, verbose=1):
        """
        Train the autoencoder model.

        Args:
            train_data (np.array): Training data
            val_data (np.array): Validation data
            epochs (int): Maximum number of epochs
            batch_size (int): Batch size for training
            verbose (int): Verbosity level

        Returns:
            dict: Training history
        """
        if self.model is None:
            raise ValueError("Model not built. Call build_model() first.")

        print(f"üöÇ Training LSTM Autoencoder:")
        print(f"   ‚Ä¢ Training samples: {len(train_data)}")
        print(f"   ‚Ä¢ Validation samples: {len(val_data)}")
        print(f"   ‚Ä¢ Max epochs: {epochs}")
        print(f"   ‚Ä¢ Batch size: {batch_size}")

        # Conservative callbacks for stable training
        early_stopping = EarlyStopping(
            monitor="val_loss",
            patience=10,  # More patience for stability
            restore_best_weights=True,
            verbose=1,
            min_delta=1e-6,
        )

        reduce_lr = ReduceLROnPlateau(
            monitor="val_loss", factor=0.5, patience=5, min_lr=1e-6, verbose=1
        )

        # Train autoencoder
        self.history = self.model.fit(
            train_data,
            train_data,  # Autoencoder: input = target
            validation_data=(val_data, val_data),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=[early_stopping, reduce_lr],
            verbose=verbose,
            shuffle=True,
        )

        # Check if training was successful
        final_loss = self.history.history["loss"][-1]
        final_val_loss = self.history.history["val_loss"][-1]

        if np.isfinite(final_loss) and np.isfinite(final_val_loss):
            print(f"‚úÖ Training successful - no NaN values")
            print(f"   ‚Ä¢ Final training loss: {final_loss:.6f}")
            print(f"   ‚Ä¢ Final validation loss: {final_val_loss:.6f}")
            return True
        else:
            print(f"‚ùå Training failed - NaN values detected")
            print(f"   ‚Ä¢ Final training loss: {final_loss}")
            print(f"   ‚Ä¢ Final validation loss: {final_val_loss}")
            return False

    def predict(self, data, verbose=0):
        """Predict/reconstruct data using the trained model."""
        if self.model is None:
            raise ValueError("Model not built. Call build_model() first.")
        return self.model.predict(data, verbose=verbose)

    def get_reconstruction_errors(self, data, verbose=0):
        """
        Compute reconstruction errors for input data.

        Args:
            data (np.array): Input data to compute errors for
            verbose (int): Verbosity level

        Returns:
            np.array: Reconstruction errors (MSE per sample)
        """
        reconstructed = self.predict(data, verbose=verbose)
        # Compute MSE per sample
        mse_per_sample = np.mean(np.square(data - reconstructed), axis=(1, 2))
        return mse_per_sample

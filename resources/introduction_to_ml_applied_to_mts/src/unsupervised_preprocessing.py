"""
Data Preparation Utilities for Unsupervised Learning

This module provides utilities for preparing time series data for LSTM autoencoders,
including sampling, validation, and conversion functions.
"""

import numpy as np
import random
import time


class UnsupervisedDataPreprocessor:
    """Data preprocessor for unsupervised learning tasks."""

    def __init__(
        self, max_training_samples=1000, max_anomaly_samples=300, random_seed=42
    ):
        """
        Initialize the data preprocessor.

        Args:
            max_training_samples (int): Maximum number of normal samples for training
            max_anomaly_samples (int): Maximum number of anomaly samples for testing
            random_seed (int): Random seed for reproducibility
        """
        self.max_training_samples = max_training_samples
        self.max_anomaly_samples = max_anomaly_samples
        self.random_seed = random_seed
        random.seed(random_seed)

    def sample_windows(self, normal_windows, anomaly_windows):
        """
        Sample windows for training efficiency.

        Args:
            normal_windows (list): List of normal operation windows
            anomaly_windows (list): List of anomaly windows

        Returns:
            tuple: (sampled_normal_windows, sampled_anomaly_windows)
        """
        print("‚ö° Smart Data Sampling for Training Efficiency")
        print("=" * 50)

        print(f"üéØ Training optimization settings:")
        print(f"   ‚Ä¢ Max normal samples for training: {self.max_training_samples}")
        print(f"   ‚Ä¢ Max anomaly samples for testing: {self.max_anomaly_samples}")

        # Sample normal data for training
        if len(normal_windows) > self.max_training_samples:
            print(
                f"üìä Sampling {self.max_training_samples} normal windows from {len(normal_windows)} available..."
            )
            sampled_indices = random.sample(
                range(len(normal_windows)), self.max_training_samples
            )
            sampled_normal_windows = [normal_windows[i] for i in sampled_indices]
        else:
            print(f"üìä Using all {len(normal_windows)} normal windows...")
            sampled_normal_windows = normal_windows

        # Sample anomaly data for testing
        if len(anomaly_windows) > self.max_anomaly_samples:
            print(
                f"üìä Sampling {self.max_anomaly_samples} anomaly windows from {len(anomaly_windows)} available..."
            )
            sampled_indices = random.sample(
                range(len(anomaly_windows)), self.max_anomaly_samples
            )
            sampled_anomaly_windows = [anomaly_windows[i] for i in sampled_indices]
        else:
            print(f"üìä Using all {len(anomaly_windows)} anomaly windows...")
            sampled_anomaly_windows = anomaly_windows

        return sampled_normal_windows, sampled_anomaly_windows

    def convert_windows_to_arrays(
        self, windows, window_type="normal", progress_step=200
    ):
        """
        Convert window DataFrames to numpy arrays with quality checks.

        Args:
            windows (list): List of window DataFrames
            window_type (str): Type of windows ("normal" or "anomaly")
            progress_step (int): Show progress every N windows

        Returns:
            list: List of valid numpy arrays
        """
        print(f"üìä Converting {window_type} windows to arrays...", end=" ")
        start_conversion = time.time()

        arrays = []
        for i, window in enumerate(windows):
            if i % progress_step == 0 and i > 0:
                print(
                    f"\rüìä Converting {window_type} windows to arrays... {i}/{len(windows)}",
                    end="",
                )

            # Get the DataFrame values and remove the class column
            if hasattr(window, "values"):
                window_data = window.copy()
                # Remove 'class' column if it exists
                if "class" in window_data.columns:
                    window_data = window_data.drop("class", axis=1)
                window_array = window_data.values
            else:
                # If it's already an array, assume last column is class and remove it
                if len(window.shape) == 2 and window.shape[1] > 1:
                    window_array = window[:, :-1]  # Remove last column (class)
                else:
                    window_array = window

            # Check for NaN or infinite values
            if np.isfinite(window_array).all():
                arrays.append(window_array)
            else:
                print(f"\n‚ö†Ô∏è Skipping {window_type} window {i} due to non-finite values")

        conversion_time = time.time() - start_conversion
        print(
            f"\rüìä Converting {window_type} windows to arrays... ‚úÖ ({len(arrays)} valid processed)"
        )

        return arrays

    def validate_and_prepare_arrays(self, normal_arrays, anomaly_arrays):
        """
        Validate array shapes and prepare final datasets.

        Args:
            normal_arrays (list): List of normal arrays
            anomaly_arrays (list): List of anomaly arrays

        Returns:
            tuple: (normal_data, anomaly_data) as numpy arrays
        """
        print("üîç Validating array shapes and data quality...")

        # Check if we have valid arrays
        if not normal_arrays or not anomaly_arrays:
            raise ValueError("No valid arrays found after conversion")

        # Get the shape from the first normal array
        expected_shape = normal_arrays[0].shape
        print(f"Expected shape: {expected_shape}")

        # Filter arrays to ensure consistent shapes
        valid_normal_arrays = []
        valid_anomaly_arrays = []

        for arr in normal_arrays:
            if arr.shape == expected_shape and np.isfinite(arr).all():
                valid_normal_arrays.append(arr)

        for arr in anomaly_arrays:
            if arr.shape == expected_shape and np.isfinite(arr).all():
                valid_anomaly_arrays.append(arr)

        print(
            f"‚úÖ Valid arrays: {len(valid_normal_arrays)} normal, {len(valid_anomaly_arrays)} anomaly"
        )

        normal_data = np.array(valid_normal_arrays, dtype=np.float32)
        anomaly_data = np.array(valid_anomaly_arrays, dtype=np.float32)

        # Data quality checks
        print(f"üìä Data quality checks:")
        print(
            f"   ‚Ä¢ Normal data range: [{np.min(normal_data):.3f}, {np.max(normal_data):.3f}]"
        )
        print(
            f"   ‚Ä¢ Anomaly data range: [{np.min(anomaly_data):.3f}, {np.max(anomaly_data):.3f}]"
        )
        print(f"   ‚Ä¢ Normal data finite: {np.isfinite(normal_data).all()}")
        print(f"   ‚Ä¢ Anomaly data finite: {np.isfinite(anomaly_data).all()}")

        return normal_data, anomaly_data

    def apply_stability_scaling(self, normal_data, anomaly_data):
        """
        Apply additional normalization for numerical stability.

        Args:
            normal_data (np.array): Normal data array
            anomaly_data (np.array): Anomaly data array

        Returns:
            tuple: (normal_scaled, anomaly_scaled)
        """
        print("üìä Additional Data Normalization for Stability")
        print("=" * 45)

        # Convert to float32 and clip extreme values for stability
        normal_scaled = normal_data.astype(np.float32)
        anomaly_scaled = anomaly_data.astype(np.float32)

        # Clip extreme values for stability
        normal_scaled = np.clip(normal_scaled, 0.001, 0.999)
        anomaly_scaled = np.clip(anomaly_scaled, 0.001, 0.999)

        print(f"üìè Enhanced data characteristics:")
        print(
            f"   ‚Ä¢ Normal data range: [{np.min(normal_scaled):.3f}, {np.max(normal_scaled):.3f}]"
        )
        print(
            f"   ‚Ä¢ Anomaly data range: [{np.min(anomaly_scaled):.3f}, {np.max(anomaly_scaled):.3f}]"
        )
        print(f"   ‚Ä¢ Data clipped to avoid extreme values")
        print(f"   ‚Ä¢ Float32 precision for stability")

        return normal_scaled, anomaly_scaled

    def prepare_full_pipeline(self, normal_windows, anomaly_windows):
        """
        Run the complete data preparation pipeline.

        Args:
            normal_windows (list): List of normal operation windows
            anomaly_windows (list): List of anomaly windows

        Returns:
            tuple: (normal_scaled, anomaly_scaled, data_info)
        """
        print("üîß Complete Data Preparation Pipeline")
        print("=" * 50)

        # Step 1: Sample windows
        sampled_normal, sampled_anomaly = self.sample_windows(
            normal_windows, anomaly_windows
        )

        # Step 2: Convert to arrays
        normal_arrays = self.convert_windows_to_arrays(sampled_normal, "normal")
        anomaly_arrays = self.convert_windows_to_arrays(
            sampled_anomaly, "anomaly", progress_step=100
        )

        # Step 3: Validate and prepare
        normal_data, anomaly_data = self.validate_and_prepare_arrays(
            normal_arrays, anomaly_arrays
        )

        # Step 4: Apply stability scaling
        normal_scaled, anomaly_scaled = self.apply_stability_scaling(
            normal_data, anomaly_data
        )

        # Get data info
        n_normal_samples, time_steps, n_features = normal_scaled.shape
        data_info = {
            "time_steps": time_steps,
            "n_features": n_features,
            "n_normal_samples": n_normal_samples,
            "n_anomaly_samples": anomaly_scaled.shape[0],
        }

        print(f"\nüìê Final Data Shapes:")
        print(f"   ‚Ä¢ Normal data: {normal_scaled.shape}")
        print(f"   ‚Ä¢ Anomaly data: {anomaly_scaled.shape}")
        print(f"   ‚Ä¢ Time steps per window: {time_steps}")
        print(f"   ‚Ä¢ Features per time step: {n_features} (class column removed)")

        return normal_scaled, anomaly_scaled, data_info
